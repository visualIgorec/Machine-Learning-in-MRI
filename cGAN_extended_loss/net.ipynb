{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90786c13-8a2f-4654-82c6-36767bc53ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision.transforms.transforms import ToPILImage\n",
    "from skimage import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318e556-57bd-4b15-9dac-a5f6ca8c394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adb637-2b23-45bb-b05a-12a0c73bcb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'D:\\\\heart_data\\\\undersampled_heart_images'\n",
    "list_dir = os.listdir(source_dir)\n",
    "list_pd = []\n",
    "for file_name in list_dir:\n",
    "    list_pd.append(file_name)\n",
    "df = pd.DataFrame(list_pd)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27fbc6-b40a-4754-80f2-616103145d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataTransform(Dataset):\n",
    "    def __init__(self, df, features_transform=None, label_transform=None):\n",
    "        self.df = df\n",
    "        self.features_transform = features_transform\n",
    "        self.label_transform = label_transform\n",
    "        self.root_dir_x = 'D:\\\\heart_data\\\\undersampled_heart_images'\n",
    "        self.root_dir_y = 'D:\\\\heart_data\\\\heart_images'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img_path_x = os.path.join(self.root_dir_x, self.df.iloc[index, 0])\n",
    "        img_path_y = os.path.join(self.root_dir_y, self.df.iloc[index, 0])\n",
    "        image_x = io.imread(img_path_x)\n",
    "        image_y = io.imread(img_path_y)\n",
    "        \n",
    "        if self.features_transform is not None:\n",
    "            image_x = self.features_transform(image_x)\n",
    "\n",
    "        if self.label_transform is not None:\n",
    "            image_y = self.label_transform(image_y)\n",
    "\n",
    "        return (image_x, image_y)\n",
    "\n",
    "x_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "#    transforms.Normalize(mean=[0.5],\n",
    "#                         std=[0.1])\n",
    "\n",
    "y_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = CustomDataTransform(df, features_transform=x_transform,\n",
    "                                  label_transform=y_transform)\n",
    "\n",
    "batch_size = 8\n",
    "part = 0.8\n",
    "train_lenght = int(len(dataset)*part)\n",
    "test_lenght = int(len(dataset) - train_lenght)\n",
    "\n",
    "train_set, test_set = random_split(dataset, [train_lenght, test_lenght])\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6a903-91b8-4eb6-929e-0bb093acacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Length of dataset is {len(dataset)}')\n",
    "plt.imshow(train_set[0][1][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4071e8-4bde-4512-8631-7063e83ea3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False, padding_mode=\"reflect\")\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.down = down\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.dropout(x) if self.use_dropout else x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=1, features=64):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, kernel_size=2, stride=1, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down2 = Block(\n",
    "            features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down3 = Block(\n",
    "            features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down4 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down5 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down6 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(features * 8, features * 8, kernel_size=4, stride=1, padding=1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.up2 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up3 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up4 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up5 = Block(\n",
    "            features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up6 = Block(\n",
    "            features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n",
    "        \n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, features * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.reeucing_conv = nn.Sequential(\n",
    "            nn.Conv2d(features * 2, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.initial_down(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        d6 = self.down5(d5)\n",
    "        #print('d6 ',d6.shape)\n",
    "        d7 = self.down6(d6)\n",
    "        #print('d7 ',d7.shape)\n",
    "\n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        #print('bottleneck',bottleneck.shape)\n",
    "        up1 = self.up1(bottleneck)\n",
    "        #print('up1',up1.shape)\n",
    "\n",
    "        up2 = self.up2(torch.cat([up1, d7], 1))\n",
    "        #print('up2',up2.shape)\n",
    "        up3 = self.up3(torch.cat([up2, d6], 1))\n",
    "        up4 = self.up4(torch.cat([up3, d5], 1))\n",
    "        up5 = self.up5(torch.cat([up4, d4], 1))\n",
    "        up6 = self.up6(torch.cat([up5, d3], 1))\n",
    "        up7 = self.up7(torch.cat([up6, d2], 1))\n",
    "        result = self.final_up(torch.cat([up7, d1], 1))\n",
    "        result = self.reeucing_conv(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2b393-2c24-40ce-bdc6-65b912507ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_all(model, init_func, *params, **kwargs):\n",
    "    for p in model.parameters():\n",
    "        init_func(p, *params, **kwargs)\n",
    "\n",
    "net = Generator(in_channels=1, features=64).to(device)\n",
    "init_all(net, torch.nn.init.normal_, mean=0.0, std=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63f424-3dd0-418a-85f6-8e0b89d77453",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_function = nn.MSELoss()  #nn.L1Loss()  nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, \n",
    "                                                       mode = 'min', \n",
    "                                                       factor = 0.9, \n",
    "                                                       patience = 200,\n",
    "                                                       threshold = 1e-4,\n",
    "                                                       verbose = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfca826-757d-413a-925b-37b827b6c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss_1(input_prediction, input_target, Loss_function):\n",
    "    return Loss_function(input_prediction, input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf04a35-bc5c-4a35-ba07-84075fa14ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_loader, net, Loss_1, train_history, optimization_step, optimizer, Loss_function):\n",
    "    for index_b, (feature, target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        feature = feature.to(device)\n",
    "        prediction = net(feature)\n",
    "        Loss = Loss_1(prediction, target, Loss_function)\n",
    "\n",
    "        optimization_step(net, optimizer, Loss)\n",
    "        train_history.append(Loss.item())\n",
    "        \n",
    "        return Loss\n",
    "\n",
    "def valid_step(test_loader, net, Loss_1, valid_history, Loss_function):\n",
    "    for index_b, (feature, target) in enumerate(test_loader):\n",
    "        target = target.to(device)\n",
    "        feature = feature.to(device)\n",
    "        prediction = net(feature)\n",
    "        Loss = Loss_1(prediction, target, Loss_function)\n",
    "        valid_history.append(Loss.item())\n",
    "\n",
    "        return Loss, prediction, target\n",
    "\n",
    "def optimization_step(net, optimizer, Loss_train_step):\n",
    "    net.zero_grad()\n",
    "    Loss_train_step.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#### SSIM Assessment\n",
    "import skimage\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "#### PSNR Assessment\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "def SSIM_PSNR_Metrcis_step(prediction, target, list_psnr, list_ssim, iteration, aver_list_psnr, aver_list_ssim):\n",
    "    sum_ssim_score = 0\n",
    "    sum_psnr_score = 0\n",
    "    for i in range(target.shape[0]):\n",
    "        ssim_score, diff = ssim(prediction.cpu().detach()[i][0].numpy(), target.cpu().detach()[i][0].numpy(), full=True,  multichannel=False)\n",
    "        #print(f\"Iter: {iteration}, \\t SSIM_score: {ssim_score}\\n\")\n",
    "        psnr_score = peak_signal_noise_ratio(image_true=target.cpu().detach()[i][0].numpy(), image_test=prediction.cpu().detach()[i][0].numpy(), data_range=None)\n",
    "        #print(f\"Iter: {iteration}, \\t PSNR_score: {psnr_score}\")\n",
    "        list_psnr.append(psnr_score)\n",
    "        list_ssim.append(ssim_score)\n",
    "        sum_ssim_score+=ssim_score\n",
    "        sum_psnr_score+=psnr_score\n",
    "    aver_list_psnr.append(sum_psnr_score/target.shape[0])\n",
    "    aver_list_ssim.append(sum_ssim_score/target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f4134-3b75-465c-bdca-8f19b1fc9443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it's useful for learning network\n",
    "epoche = 1200\n",
    "\n",
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "list_psnr = []\n",
    "list_ssim = []\n",
    "aver_list_psnr = []\n",
    "aver_list_ssim = []\n",
    "\n",
    "for iteration in range(epoche):\n",
    "\n",
    "    #train and valid step\n",
    "    Loss_train_step = train_step(train_loader, net, Loss_1, train_history, optimization_step, optimizer, Loss_function)\n",
    "    Loss_valid_step, prediction, target = valid_step(test_loader, net, Loss_1, valid_history, Loss_function)\n",
    "\n",
    "    #Reducing step of lr\n",
    "    scheduler.step(Loss_train_step)\n",
    "    print(scheduler.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    #image metrics\n",
    "    #SSIM_PSNR_Metrcis_step(prediction, target, list_psnr, \n",
    "    #                       list_ssim, iteration,\n",
    "    #                       aver_list_psnr,\n",
    "    #                       aver_list_ssim)\n",
    "\n",
    "    print(f'Epoche â„–: {iteration}')\n",
    "    \n",
    "    #Visualization\n",
    "    img_test = prediction.cpu().detach()[0][0]\n",
    "    img_target = target.cpu().detach()[0][0]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img_test, cmap='gray')\n",
    "    plt.title(\"Test image\")\n",
    "\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(img_target, cmap='gray')\n",
    "    plt.title(\"Target image\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Loss_train_step per batch: {Loss_train_step}\\nLoss_valid_step per batch: {Loss_valid_step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d312bd-7b46-4daf-8f68-f37529d1d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_set))\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452701fc-0cd7-4b09-8590-e179303020b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Valid Error Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_history, label = 'Train Loss: ' + str(round(train_history[-1], 3)))\n",
    "ax.plot(valid_history, label = 'Test Loss: ' + str(round(valid_history[-1], 3)))\n",
    "ax.legend(fontsize=15)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(12)\n",
    "plt.grid()\n",
    "plt.title(\"Error History\", fontsize= 20, fontweight='bold')\n",
    "plt.xlabel(\"Epoches\", fontsize= 20)\n",
    "plt.ylabel(\"Error\", fontsize= 20)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf68de1-a2e9-4cf0-89bd-45247b7afd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "PATH = '/content/gdrive/MyDrive/Medical_Imaging/DiplomaReconstructionImage/LearnedNetwork/UNetRec_Mode_Extended_Loss_v1.pt'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
